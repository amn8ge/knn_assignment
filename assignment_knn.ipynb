{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f7ef20f0-722f-4240-8a79-437d4a3b8832",
      "metadata": {
        "id": "f7ef20f0-722f-4240-8a79-437d4a3b8832"
      },
      "source": [
        "## Assignment 3: $k$ Nearest Neighbor\n",
        "\n",
        "`! git clone https://github.com/ds3001f25/knn_assignment.git`\n",
        "\n",
        "**Do two questions in total: \"Q1+Q2\" or \"Q1+Q3\"**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d9212c0",
      "metadata": {
        "id": "5d9212c0"
      },
      "source": [
        "**Q1.**\n",
        "1. What is the difference between regression and classification?\n",
        "\n",
        "A regression involves predicting a numerical outcome and classification is predicting a categorical outcome. Regression predicts a continuous, numerical output like price or temperature, while classification assigns data to discrete, categorical labels like legal or illegal.\n",
        "\n",
        "2. What is a confusion table? What does it help us understand about a model's performance?\n",
        "\n",
        "A confusion table is a cross-tabulation of predicted and actual values that we implement to analyze classifications. It helps us understand how many predictions were correct versus incorrect and identify specific areas where the model struggled. As a result, it is one of the most fundamental metrics for whether a model has done well or poorly.\n",
        "\n",
        "3. What does the SSE quantify about a particular model?\n",
        "\n",
        "The SSE (sum of squared errors) quantifies the sum of the squared difference from the predicted dependent y value to the true outcome for each point in the validation set. It represents the unexplained variability—the portion of the total variability in the dependent variable that is not explained by the regression model.\n",
        "\n",
        "4. What are overfitting and underfitting?\n",
        "\n",
        "Overfitting and underfitting reference the impact of sizing k differently—if k is very small, the model is unrealistically precise, which is referred to as overfitting. The opposite end of the spectrum is when k is very large, and the model is unrealistically imprecise. This is referred to as underfitting.\n",
        "\n",
        "5. Why does splitting the data into training and testing sets, and choosing $k$ by evaluating accuracy or SSE on the test set, improve model performance?\n",
        "\n",
        "Splitting data into training and testing sets helps prevent overfitting by ensuring that the model is evaluated on unseen data. This way, we get an unbiased estimate of performance. Selecting k based on accuracy or SSE on the test set helps identify the value that works well not just on training data but also new, unseen data. This practice empowers us to select the best-performing $k$ by helping avoid both overfitting and underfitting and being confident in the real world performance of the model.\n",
        "\n",
        "6. With classification, we can report a class label as a prediction or a probability distribution over class labels. Please explain the strengths and weaknesses of each approach.\n",
        "\n",
        "Predicting a class labels are easy to interpret and immediate results, whereas reporting probabilities is a more in depth treatment. Potential uncertainties are ignored when predicting a class label. Probability distributions indicate model confidence and uncertainty, which is helpful when making nuanced or final decisions—however, in some cases, especially non-expert ones, it probably makes more sense to use the predicted class label because it will be more easily digested. Additionally, probabilities require careful calibration to ensure they reflect true likelihoods, and misuse can lead to significant decision errors."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "194455fa",
      "metadata": {
        "id": "194455fa"
      },
      "source": [
        "**Q2.** This question is a case study for $k$ nearest neighbor regression, using the `USA_cars_datasets.csv` data.\n",
        "\n",
        "The target variable `y` is `price` and the features are `year` and `mileage`.\n",
        "\n",
        "1. Load the `./data/USA_cars_datasets.csv`. Keep the following variables and drop the rest: `price`, `year`, `mileage`. Are there any `NA`'s to handle? Look at the head and dimensions of the data.\n",
        "2. Maxmin normalize `year` and `mileage`.\n",
        "3. Split the sample into ~80% for training and ~20% for evaluation.\n",
        "4. Use the $k$NN algorithm and the training data to predict `price` using `year` and `mileage` for the test set for $k=3,10,25,50,100,300$. For each value of $k$, compute the mean squared error and print a scatterplot showing the test value plotted against the predicted value. What patterns do you notice as you increase $k$?\n",
        "5. Determine the optimal $k$ for these data.\n",
        "6. Describe what happened in the plots of predicted versus actual prices as $k$ varied, taking your answer into part 6 into account. (Hint: Use the words \"underfitting\" and \"overfitting\".)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "739292a2",
      "metadata": {
        "id": "739292a2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "8d193de6",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "8d193de6"
      },
      "source": [
        "**Q3.** This is a case study on $k$ nearest neighbor regression and imputation, using the `airbnb_hw.csv` data.\n",
        "\n",
        "There are 30,478 observations, but only 22,155 ratings. We're going to build a kNN regressor to impute missing values. This is a common task, and illustrates one way you can use kNN in the future even when you have more advanced models available.\n",
        "\n",
        "1. Load the `airbnb_hw.csv` data with Pandas. We're only going to use `Review Scores Rating`, `Price`, and `Beds`, so use `.loc` to reduce the dataframe to those variables.\n",
        "2. Set use `.isnull()` to select the subset of the dataframe with missing review values. Set those aside in a different dataframe. We'll make predictions about them later.\n",
        "3. Use `df = df.dropna(axis = 0, how = 'any')` to eliminate any observations with missing values/NA's from the dataframe.\n",
        "4. For the complete cases, create a $k$-NN model that uses the variables `Price` and `Beds` to predict `Review Scores Rating`. How do you choose $k$? (Hint: Train/test split, iterate over reasonable values of $k$ and find a value that minimizes SSE on the test split using predictions from the training set.)\n",
        "5. Predict the missing ratings.\n",
        "6. Do a kernel density plot of the training ratings and the predicted missing ratings. Do they look similar or not? Explain why."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b362aa0d",
      "metadata": {
        "id": "b362aa0d"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}